# IOT Drum Set
This is a personal project for **IM5032 – Practical Internet of Things**.

Unlike guitars, drums are not easily portable, and existing air drumsticks on the market are relatively expensive.    
Therefore, this project aims to implement a **simple and affordable air drumstick** using a **Raspberry Pi 4** and an **MPU6050 sensor**.  
  
**Most of the code were generated by Claude and Gemini.**

#### Game View
<img src="readme_img/image.png" alt="IOT Drum Set" width="100%">

#### Hard Wares 
<img src="readme_img/full.jpg" alt="IOT Drum Set" width="100%">


## Demo Video

[![Demo Video](readme_img/demo.png)](https://youtu.be/Q8ORrFAGLZA?si=hhA_naFbbZAaaGCb)

## Required Components
| Item | Quantity | Purpose |
|-----|----------|---------|
| Raspberry Pi 4  | 1 | Main controller for data processing and system control |
| MPU6050  | 2 | Detect drumstick motion |
| Drumstick | 2 | Otherwise it can't be an air drumstick : ) |
| Breadboard | 1 | For better wiring |
| Jumper wires  | many | Connect MPU6050 to Raspberry Pi GPIO |
| Tape  | many | Make everything neat |
| Drum Sounds  | many | Can visit here: https://elevenlabs.io/sound-effects |


## Circuit Diagram
<div align="center">
  <img src="readme_img/Wiring.jpg" alt="IOT Drum Set" width="100%">
</div>


## Getting Start
### 1. Environment Setup

#### Install System Dependencies
```bash
sudo apt-get update
sudo apt-get install i2c-tools python3-smbus ffmpeg
```

#### Required Python Packages
`flask`  `flask-cors`  `smbus2`  `mpu6050-raspberrypi`  `flask-socketio`  `python-socketio`  `eventlet`  

---

### 2. MPU6050 Sensor Setup

This section explains how to connect **one or two MPU6050 sensors** to a Raspberry Pi.

The MPU6050 is a 6-axis IMU sensor (3-axis accelerometer + 3-axis gyroscope), which can detect movement and rotation in all directions.  
It is important to understand the relationship between the sensor axes and real-world directions, as this directly affects motion interpretation.

<div align="center" style="display: flex; gap: 10px;">
  <img src="readme_img/mpu6050.png" alt="MPU6050 module" width="50%">
  <img src="readme_img/mpu6050_axis.png" alt="MPU6050 axis orientation" width="50%">
</div>

<br>
<br>

#### Enable I2C Interface

First, open the Raspberry Pi configuration tool and enable **I2C** and **SPI**:

<img src="readme_img/setting.png" alt="I2C detection result" width="100%">

#### Install I2C Tools and Detect Sensor

Install I2C tools:

    sudo apt-get install i2c-tools

Check whether the Raspberry Pi can detect the sensor:

    i2cdetect -y 1

If your Raspberry Pi uses a different I2C bus, try:

    i2cdetect -y 0

If `0x68` or `0x69` appears as shown below, the sensor is successfully detected.

<img src="readme_img/I2C.png" alt="I2C detection result" width="100%">

#### Connecting One MPU6050 Sensor

By default, the MPU6050 uses I2C address `0x68`.

| MPU6050 | Raspberry Pi |
|--------|--------------|
| VCC    | 3.3V / 5V    |
| GND    | GND          |
| SDA    | GPIO 2       |
| SCL    | GPIO 3       |
| AD0    | GND          |

After wiring, run `i2cdetect -y 1`.  If `0x68` appears, the sensor is connected correctly.

#### Connecting Two MPU6050 Sensors

To connect two MPU6050 sensors on the same I2C bus, their I2C addresses must be different.

- Sensor 1: `AD0 → GND` → Address `0x68`
- Sensor 2: `AD0 → VCC` → Address `0x69`

Both sensors share the same SDA and SCL lines.

| Signal | Sensor 1 | Sensor 2 | Raspberry Pi |
|--------|----------|----------|--------------|
| VCC    | VCC      | VCC      | 3.3V / 5V    |
| GND    | GND      | GND      | GND          |
| SDA    | SDA      | SDA      | GPIO 2       |
| SCL    | SCL      | SCL      | GPIO 3       |
| AD0    | GND      | VCC      | —            |

After wiring, run `i2cdetect -y 1`.
You should see **both `0x68` and `0x69`**, indicating that two sensors are detected successfully.

You can also refer to the following tutorial for more details:  [https://atceiling.blogspot.com/2017/02/raspberry-pi-mpu-6050.html](https://atceiling.blogspot.com/2017/02/raspberry-pi-mpu-6050.html)



---


### 3. Calibrate Sensor Data
The offset values were obtained by collecting data while the sensors were **stationary and flat** on a table, then calculating the average error over multiple samples.

**Method:**

1. **Place the sensor flat on a stable surface** (drumstick lying horizontally on a table)

2. **Run the test script**  `mpu6050_test.py` to collect raw data. This script will record 100-200 samples of raw accelerometer and gyroscope readings while stationary

3. **Calculate  the average values** and the offest values of each sensor will be updated and used in `calibration_right.py` and `calibration_left.py`

---

### 4. 3D World Setup

The following figure illustrates the setup of the 3D world and the mapping between the sensor directions and the virtual drum set.

<img src="readme_img/3D_settings.jpg" alt="3D world and axis mapping" width="100%">

#### Drum Position Settings
```js
const zones = [
  { name: "Hihat", x: 675, y: 225, w: 225, h: 225, color: "#3232ff", pos3d: [1.8, 0.8, -1], radius: 0.65, rotation: -Math.PI / 9, glowColor: "#3399ff" },
  { name: "Snare", x: 450, y: 225, w: 225, h: 225, color: "#d9d9d9", pos3d: [0.5, 0.4, -1], radius: 0.65, rotation: -Math.PI / 12, glowColor: "#ffffff" },
  { name: "Tom_high", x: 450, y: 0, w: 225, h: 225, color: "#ff7f2a", pos3d: [0.6, 0.8, 0.3], radius: 0.5, rotation: -Math.PI / 5, glowColor: "#ff6600" },
  { name: "Tom_mid", x: 450, y: 0, w: 225, h: 225, color: "#ff7f2a", pos3d: [-0.6, 0.8, 0.3], radius: 0.5, rotation: -Math.PI / 5, glowColor: "#ff6600" },
  { name: "Symbal", x: 675, y: 0, w: 225, h: 225, color: "#e5b3ff", pos3d: [1.7, 1.4, 0.5], radius: 0.80, rotation: -Math.PI / 6, glowColor: "#ff00ff" },
  { name: "Ride", x: 0, y: 0, w: 225, h: 225, color: "#6eeee7", pos3d: [-1.8, 1.4, -0.1], radius: 0.90, rotation: -Math.PI / 6, glowColor: "#00ffff" },
  { name: "Tom_floor", x: 675, y: 225, w: 225, h: 225, color: "#4d4d4d", pos3d: [-1.2, 0.2, -1], radius: 0.80, rotation: -Math.PI / 9, glowColor: "#aaaaaa" },
];
```

#### 3D World
This project uses **Three.js** (https://threejs.org/) to create the invironment and the virtual drum set.  
Due to time limitations, the bass drum is not included in the current implementation. 

To simplify motion interpretation, it would have been easier to predefine the axis mapping between the MPU6050 sensor and the 3D world.  
Be careful, the coordinate system of the 3D world is fixed and does **not change with the camera position**.

From the user’s point of view, the relation of each data is as follows:

| Movement | 3D World Mapping | Sensor Data |
|---------|------------------|-------------|
| Left–Right Movement | X-axis | Yaw (rotation around Z-axis) |
| Up–Down Movement | Y-axis | Pitch (rotation around Y-axis) |
| Forward–Backward Movement | Z-axis | Pitch (rotation around Y-axis) + ax (X-axis acceleration) |
| Vertical Drumstick Swing (Hitting)| rotation x | Pitch (rotation around Y-axis) + gy (Y-axis angular velocity) |
| Horizontal Drumstick Swing | rotation y | Yaw (rotation around Z-axis) |

### 5. Hitting Detection
#### Detection Flow
Sensor Data → Hit Detection → Collision Detection → Identify Target Drum → Play Sound

A drum hit is detected when **both conditions** are met:

```python
# Condition 1: Rapid downward swing (gyroscope Y-axis)
is_downward_swing = abs(gy) > 50  # Angular velocity > 50°/s

# Condition 2: Impact acceleration detected
has_acceleration = abs(az) > 0.5 or abs(ax) > 0.5  # Acceleration > 0.5g

# Hit detected when BOTH are true
is_hit = is_downward_swing and has_acceleration
```
#### Drum Detection
It's difficult to accurately implement sensor's data to precise movement and strike angles in the 3D scene.  
Also, since each drum has a different height and a tilted surface, simple position-based collision detection is not sufficient.  
Therefore, this project introduces **surface normal vectors** for each drum surface and defines a **strike air zone** above the drum head.

For each drum:
- The **position of the drum surface** is defined in the 3D world
- A **surface normal vector** is calculated to represent the drum’s orientation
- An invisible **air zone** is constructed along the normal direction above the drum surface

When hitting is detected, the system will see which **air zone** the drum head is in, then play the corresponding drum sound.

---


### 6. Hand and Drumstick Movement

The drumstick is composed of two parts: the **head** and the **hand**, as shown in the figure below.

To avoid unnatural or unstable animations, a **movement zone** is defined for the drumstick head.  This zone is centered around the surface center of each drum. (see green area in the picture)  The head of the drumstick is only allowed to move within this zone.  Based on the fixed length of the drumstick, the movement zone of the hand can then be calculated accordingly.

Note, the **z-value of the hand is always smaller than that of the head**, ensuring that the hand remains behind the striking point during motion.
 
<img src="readme_img/move_zone.jpg" alt="3D world and axis mapping" width="100%">

``` js
function calculateGripRanges() {
    // Extract X and Z positions from all drum surfaces
    const drumXPositions = zones.map(z => z.pos3d[0]);
    const drumZPositions = zones.map(z => z.pos3d[2]);
    
    // Determine the overall X and Z boundaries of the drum set
    const minDrumX = Math.min(...drumXPositions);  //  (Ride)
    const maxDrumX = Math.max(...drumXPositions);  // (Hi-hat)
    const minDrumZ = Math.min(...drumZPositions);  //  (rear drums)
    const maxDrumZ = Math.max(...drumZPositions);  //  (Cymbal)
    
    // Calculate the Z-axis movement range for the hand (grip)
    // Hand position = drum surface position - stick length * scaling factor
    const GRIP_Z_MIN = minDrumZ - STICK_LENGTH * 0.8;
    const GRIP_Z_MAX = maxDrumZ - STICK_LENGTH * 0.7;
    
    // Calculate the X-axis movement range for the right hand
    // preventing it from reaching the drumstick tip
    const GRIP_RIGHT_X_MIN = GRIP_RIGHT_X + (minDrumX - GRIP_RIGHT_X) * 0.6;
    const GRIP_RIGHT_X_MAX = GRIP_RIGHT_X + (maxDrumX - GRIP_RIGHT_X) * 0.6;
    
    return { GRIP_Z_MIN, GRIP_Z_MAX, GRIP_RIGHT_X_MIN, GRIP_RIGHT_X_MAX };
}
```

## References
* DIY Air Drum Arduino (https://www.youtube.com/watch?v=tQ3_pOlgquQ)
* IMU-algorithm (https://github.com/rbv188/IMU-algorithm)
* ElevenLabs (https://elevenlabs.io/sound-effects)
* ECE 5730 Final Project: Air Drums (https://ece4760.github.io/Projects/Fall2023/ds2392_kx74_ac2839/Air_Drum.html)
* 結合MPU6050 三軸陀螺儀，創建模擬飛機儀表 (https://www.youtube.com/watch?v=6L_Y4Qsh41o)
* Rotate 3d Object in Unity (https://www.youtube.com/watch?v=zN89M_MjVKo)
* MPU-6050 Six-Axis unity 3d and Arduino (https://www.youtube.com/watch?v=L4WfHT_58Dg)
* Web Server with MPU-6050 Accelerometer and Gyroscope (https://www.youtube.com/watch?v=dXcF-Uqa-gw&t=15s)

## Failed Attempts

#### Using Unity 3D
Unity was initially used to visualize the drumset and drumstick motion, as it's easier to adjust the scene position. However, later we found it's more smoother to visulize on web, through flask. 

#### Hitting Zone Optimization
Early attempts mapped drum surfaces onto the XZ plane (floor) or directly onto the camera view, but both ended up with weird movement and unstable hit detection.

## Reflections

I am glad to have the opportunity to work hands-on on this project.  
Although it took a considerable amount of time, it reminded me how enjoyable the brainstorming process is and how interesting coding can be.
This is also my first project that is fully based on AI. Through this experience, I learned that breaking a complex task into smaller blocks is far more effective than tackling everything at once, and that having a clear blueprint in mind before implementation is essential.  
After countless nights spent refining the animation and motion accuracy, I finally understand why commercial air drumsticks are so expensive :)
Happy to say that I have a lot of fun with this project !!

